{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a38ea0-13f6-4c3f-847c-220bc5cf1eb8",
   "metadata": {},
   "source": [
    "# Employee work life analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ba4fd-9c6b-4a0e-b623-6dbc6faf3fbe",
   "metadata": {},
   "source": [
    "## Introduction\r\n",
    "In the pursuit of understanding the intricacies of a company's culture and the experiences of its employees, the extraction and summarization of work-life anecdotes emerge as a valuable undertaking. In this phase of the project, we delve into the narratives of individuals who have been part of various organizations, gathering insights that encapsulate the essence of their professional journeys. To accomplish this, we employ a combination of web scraping, data extraction, and text summarization techniques, augmented by popular libraries and methodologies in the realm of Natural Language Processing (NLP). By employing these methods, we aim to provide concise and insightful summaries of employees' experiences, enabling a deeper comprehension of the working environments, challenges, and successes that define a company's culture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e38b4e-a881-4047-a8bc-3462a9bb69d7",
   "metadata": {},
   "source": [
    "## Web Scraping Quora blog posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c127a5-2cfe-4f5c-b525-71adfdd52f37",
   "metadata": {},
   "source": [
    "### Input the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d1cf0f-f00a-4e54-8cf3-52d00f9ba218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which company's employee experiences do you want to know about? facebook\n"
     ]
    }
   ],
   "source": [
    "company = input(\"Which company's employee experiences do you want to know about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aaa2b99-86c9-4120-8e6c-bb42b74c48cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a matching div element:\n",
      "The Perks I can clearly remember how strange it all felt at first. I'm from a small Canadian prairie city and had spent the majority of my career working in art galleries where we were always strapped for cash. Facebook, on the other hand, is a place designed so that their employees can focus entirely on productivity. Breakfast, lunch and dinner are available for free, there's an onsite dry cleaning drop off so you don't have to worry about laundry, the shuttle picks you up in the morning and jets you off to work as you catch up on email using the Wi-Fi in the leather-seated vehicle ... For about six months after I started, I walked around completely astonished at how lucky I was, wrapped in a bubble of gratitude. Eventually these perks start to feel normal and although they're what people talk about most often, they're not the reason I stuck around for four years.  The Hustle I've never worked harder in my life than I did at Facebook. I put in long days, I often logged in on weekends and I felt a tremendous sense of responsibility to do a good job. The day-to-day environment at Facebook is unstructured. No one really tells you what to do and when to do it and to a large extent, it's up to you to hustle to make sure you're invited to the right meetings and spending your time wisely. I appreciated the freedom and flexibility, but I also struggled a bit with the lack of process. As a content strategist, my role wasn't as well defined or understood as other design roles and sometimes it was challenging to find my way into a project in a meaningful way. The Structure Facebook is a flat organization. There are managers (and I was one for awhile), but their job is really to figure out how to remove barriers so people can be successful in their roles. The manager/non-manager relationship is characterized as \"separate but equal\" and the prevailing sentiment is that becoming a manager is a career change, not a career advancement. As a content strategist, I was regularly in product reviews with people like Sheryl Sandberg, Boz, Chris Cox and even Mark Zuckerberg. The good part of this is that it's possible to have an impact and persuade people to listen to your ideas. It's incredibly gratifying to see something you advocated for launched or a small improvement to an experience make a big difference in terms of people's ability to understand a feature. The negative side of a flat structure is that no one needs to listen to you or involve you in anything, so it's up to you to lobby for your ideas and your role. If you end up spending a lot of time pushing, it can get to be exhausting.  The Team I can't speak about other organizations at Facebook, but I will say that the content strategy team was the smartest, most generous group of people I've ever had the pleasure of working with. I learned more with them in four years then in the previous ten years of my career. \n",
      "---\n",
      "Found a matching div element:\n",
      "Summary: It is awesome being a Data Scientist at Facebook. There are multiple analytics teams at Facebook. I manage a team of Data Scientists and analysts working on Ads and we belong to probably the largest and most centralized analytics team at Facebook. Our goal is to come up with data backed insights which will result in informing the product road-map or move key metrics that our product teams track. We sometimes also build infrastructure (less common in my world) that are used by other Data Scientists and engineers. We work in close concert with Engineering and Product and we often wear Engineering or Product management hats in addition to our Data Scientist responsibilities. We spend our time in: A: Analyzing and designing experiments to optimize product features or move key metrics B: Data mining/analysis to come up with business opportunities to pursue or product feature suggestions or sometimes to understand metric movements. C: Building production ML models (though this is mostly done by SW Engineering) The multidisciplinary nature of the role, access to one of the largest troves of data, brilliant colleagues and ability to create a huge impact in a very short time period make this an exciting job.\n",
      "---\n",
      "Found a matching div element:\n",
      "I recently worked at Facebook. It was horrendous. I can’t speak for the entire company, but my organization was a disaster. The pay and benefits were outstanding, so please believe me when I say that I quit because each day felt like mental torture. For starters, gross ineptitude extended all the way up my management chain. The people I worked for had no idea what they were doing apart from distributing corporate kool-aid and acting like used car salesmen. It didn’t help that the lower level employees in my org typically had many years of industry experience and/or advanced degrees. Have you ever had someone with zero clue how your industry works micromanage you 24 hours a day? How about 3–4 levels of clueless managers trying to control your every move while piling on mountains of useless work to support their productivity metrics? Like I said, torture. Call me crazy, but the members of our team with copious amounts of experience and qualifications should probably have been seen as capable enough to share insights and constructive criticism… but not at Facebook! At Facebook, everything was top-down directed and already “world-class.” Any employee who tried to fix a problem or improve a workflow without a mandate was treated like an insider threat, apparently for acknowledging a flaw. My management was always more interested in coating turds in tinfoil to sell to the C Suite as platinum instead of allowing us to produce anything worthwhile. It was a recipe for poor work-life balance and nefarious micromanagement on top of a bunch of truly meaningless busy work. The performance review system was probably to blame for much of the pressure for employees to act like everything was perfect at all times. It ensured that workers were constantly stressed about losing their jobs (a certain number automatically failed each cycle) and was not effective at actually rooting out bad apples. Anyone with a decent write-up and enough social connections could sail through, since managers and peers had to act as advocates during the review. This meant that avoiding rocking the boat and brown-nosing were the key skills rewarded, far above any actual value added to operations. The pervasive fear of voicing criticism or upsetting managers suppressed innovation and made it impossible to fix the multitude of glaring problems. Many folks were even too afraid to speak up on the company’s internal climate surveys because enough information was shared with managers to potentially reveal who said what. People have described Facebook as cult-like, but it felt more like a dystopia to me. Apart from my job being useless, my management and coworkers largely being a bunch of fake back-stabbers, and feeling like I had to constantly watch out for the thought police, the company was also always in the news for negative reasons. It was awful. People around the world never stopped finding new ways to abuse the platforms, but the company would just push forward with the next big innovation or acquisition to stay ahead of the earnings curve. Many of my colleagues did not care at all about the consequences of our operations and were incapable of accepting that any outside criticism of the company could be valid. People in X country are using Whatsapp to spread lies that resulted in a lynch mob? Who cares, we are morally superior because we are bringing people together and making truckloads of cash! Facebook had a very high turnover rate while I was there, and I knew I would not be able to tolerate it long term as soon as the excitement about free food in the office wore off. That said, I don’t believe Facebook deserves quite as much criticism as it gets (just a smidge) because the task of preventing humans from being evil online at such a massive scale is truly monumental. Still, I think Zuck needs to step aside so a new range of responsible leaders can take the reins to increase internal accountability and cleanse the garbage employees and toxic culture. The reality is that social media platforms are incredibly powerful and can be used to bring criminals, terrorists, and nation state propaganda programs closer to their targets just as easily as they can connect groups of friends. These are not tools to be taken lightly or managed in a half-assed fashion by self-serving corporate hacks.\n",
      "---\n",
      "Found a matching div element:\n",
      "It’s different than working for any other company. You are surrounded by super-smart people, there’s no hierarchy, everyone is aligned with the mission of the company and you get a lot of respect from the outside world for working for a company like Facebook. They do have a lot of perks and luxuries for their employees, but those don’t matter as much the top management’s commitment to company’s mission and employee well-being! You will get all the resources to execute your job well. Also, the company maintains the highest levels of transperancy in the industry. You’ll have access to entire product roadmaps, dashboard, you can share your feedback and ideas on anything, access top management including Zuckerberg if you have something relevant to discuss or share. There’s a lot of flexibility that FB offers - work from home, 4 months later Ety leaves, unlimited sick days etc. If you have an offer from FB, you should seriously evaluate and not let it go!\n",
      "---\n",
      "Found a matching div element:\n",
      "Facebook Hyderabad is one of the best Facebook offices to work at. It's good because people are really friendly and everyone takes time to know each other. Also it has the best facilities. Nowhere in the world, a Facebook employee is picked up from their doorstep at a time which is convenient to them.  The only bad side of working at Facebook Hyderabad is that sometimes one might feel a little detached from the bigger picture that Facebook is creating globally. This is not a problem specific to Facebook Hyderabad, but in general to any remote office. Everyone at Facebook do make an effort to make people in the remote offices realize how valuable they are and so it is not much of a problem. Lastly, working at a remote office means an opportunity to travel to various locations. That surely covers up for whatever you are missing!\n",
      "---\n",
      "Found a matching div element:\n",
      "I worked at FB as a contractor from 2010-2012.  I can only speak about my experience during that time frame.   Facebook for me was like boot camp that prepared me for my career.  I am a Facilities Operations Manager.  My team and I supported the day to day operations at Facebook.  We also supported special events.  Some included but not limited to; Hackathons, F8, Product launches, visits from PODUS Barack Obama, Ex-President G.W. Bush, Ex-Vice President Al Gore, and various celebrities.  During my tenure there I was treated very well.  I never felt like a second class citizen and was allowed to partake in various programs.  I was allowed to eat in all their cafes for free, participated in their transportation program, and attended various functions.  Facebook really tried to make everyone feel like part of the family.  Whenever I participated in meetings my views and opinions were respected and felt like I contributed to the success of my organization. If you want to work at Facebook and have an opportunity to go as a contractor, go for it!\n",
      "---\n",
      "Found a matching div element:\n",
      "Keep in mind that Facebook does not outright hire \"Java Engineers\" or \"C++ Engineers\". Engineers are hired as generalists, and then happen to work in various technologies when the project requires it. So there really are no \"Java Developers\" at Facebook.  What's it like for the people who spend a good amount of their time in Java (aka Android, etc)? Good? I'm not sure the answer you're looking for here. If you're looking for a list of tools they use to make their life here, checkout the Mobile @ Scale conference that Facebook hosts. Checkout this video for some of our Android tools we publicly talk about.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = f\"https://www.quora.com/search?q=what%20is%20it%20like%20working%20at%20{company}\"\n",
    "\n",
    "# Initialize a headless browser \n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"qu-userSelect--text\")))\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "button_elements = driver.find_elements(By.XPATH, \"//div[@class='q-text qu-cursor--pointer QTextTruncated__StyledReadMoreLink-sc-1pev100-3 dXJUbS qt_read_more qu-color--blue_dark qu-fontFamily--sans qu-pl--tiny']\")\n",
    "\n",
    "company_blog_posts=[]\n",
    "for button_element in button_elements[:10]:\n",
    "    button_element.click()  \n",
    "    \n",
    "page_source = driver.page_source\n",
    "# Get the page source after dynamic content has loaded\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "div_elements = soup.find_all('div', class_='q-box spacing_log_answer_content puppeteer_test_answer_content')\n",
    "for div_element in div_elements[:10]:\n",
    "    company_blog_posts.append(div_element.get_text())\n",
    "\n",
    "for c in company_blog_posts:\n",
    "    print(\"Found a matching div element:\")\n",
    "    print(c)\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be1989-1c6c-44e7-9c21-f30eeea0bc0c",
   "metadata": {},
   "source": [
    "Note that we have extracted raw blog posts and have stored it in company_blog_posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77c0a0-b489-4fe6-ac24-3f0819120bd9",
   "metadata": {},
   "source": [
    "## Pre-Processing of extracted data (Removal of stop words and tokenisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9feed6-f935-4288-8c20-0a2269697f44",
   "metadata": {},
   "source": [
    "In my code, I have incorporated the NLTK library's Stop words and Punkt tokenizer for effective text processing. \n",
    "<br/> **Stop words** removal plays a crucial role in enhancing the quality of textual analysis by filtering out common words that carry little semantic meaning, thus allowing a focus on more significant terms. \n",
    "<br/> **Punkt Sentence Tokenizer** helps to process the abbreviation words, upper case characters, collocations, special characters, whitespaces and many more, making it particularly effective summarisation.\n",
    "\n",
    "These preprocessing steps contribute to the efficiency and precision of natural language processing tasks, ensuring a more refined understanding of the underlying text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f58dec7-868a-4b45-a55d-1c2e18242998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed Individual Blogs\n",
      "1: perks clearly remember strange felt first small canadian prairie city spent majority career working art galleries always strapped cash facebook hand place designed employees focus entirely productivity breakfast lunch dinner available free onsite dry cleaning drop worry laundry shuttle picks morning jets work catch email using vehicle six months started walked around completely astonished lucky wrapped bubble gratitude eventually perks start feel normal although people talk often reason stuck around four years hustle never worked harder life facebook put long days often logged weekends felt tremendous sense responsibility good job environment facebook unstructured one really tells large extent hustle make sure invited right meetings spending time wisely appreciated freedom flexibility also struggled bit lack process content strategist role well defined understood design roles sometimes challenging find way project meaningful way structure facebook flat organization managers one awhile job really figure remove barriers people successful roles relationship characterized separate equal prevailing sentiment becoming manager career change career advancement content strategist regularly product reviews people like sheryl sandberg boz chris cox even mark zuckerberg good part possible impact persuade people listen ideas incredibly gratifying see something advocated launched small improvement experience make big difference terms people ability understand feature negative side flat structure one needs listen involve anything lobby ideas role end spending lot time pushing get exhausting team ca speak organizations facebook say content strategy team smartest generous group people ever pleasure working learned four years previous ten years career\n",
      "\n",
      "2: summary awesome data scientist facebook multiple analytics teams facebook manage team data scientists analysts working ads belong probably largest centralized analytics team facebook goal come data backed insights result informing product move key metrics product teams track sometimes also build infrastructure less common world used data scientists engineers work close concert engineering product often wear engineering product management hats addition data scientist responsibilities spend time analyzing designing experiments optimize product features move key metrics b data come business opportunities pursue product feature suggestions sometimes understand metric movements c building production ml models though mostly done sw engineering multidisciplinary nature role access one largest troves data brilliant colleagues ability create huge impact short time period make exciting job\n",
      "\n",
      "3: recently worked facebook horrendous speak entire company organization disaster pay benefits outstanding please believe say quit day felt like mental torture starters gross ineptitude extended way management chain people worked idea apart distributing corporate acting like used car salesmen help lower level employees org typically many years industry experience advanced degrees ever someone zero clue industry works micromanage 24 hours day levels clueless managers trying control every move piling mountains useless work support productivity metrics like said torture call crazy members team copious amounts experience qualifications probably seen capable enough share insights constructive facebook facebook everything directed already employee tried fix problem improve workflow without mandate treated like insider threat apparently acknowledging flaw management always interested coating turds tinfoil sell c suite platinum instead allowing us produce anything worthwhile recipe poor balance nefarious micromanagement top bunch truly meaningless busy work performance review system probably blame much pressure employees act like everything perfect times ensured workers constantly stressed losing jobs certain number automatically failed cycle effective actually rooting bad apples anyone decent enough social connections could sail since managers peers act advocates review meant avoiding rocking boat key skills rewarded far actual value added operations pervasive fear voicing criticism upsetting managers suppressed innovation made impossible fix multitude glaring problems many folks even afraid speak company internal climate surveys enough information shared managers potentially reveal said people described facebook felt like dystopia apart job useless management coworkers largely bunch fake feeling like constantly watch thought police company also always news negative reasons awful people around world never stopped finding new ways abuse platforms company would push forward next big innovation acquisition stay ahead earnings curve many colleagues care consequences operations incapable accepting outside criticism company could valid people x country using whatsapp spread lies resulted lynch mob cares morally superior bringing people together making truckloads cash facebook high turnover rate knew would able tolerate long term soon excitement free food office wore said believe facebook deserves quite much criticism gets smidge task preventing humans evil online massive scale truly monumental still think zuck needs step aside new range responsible leaders take reins increase internal accountability cleanse garbage employees toxic culture reality social media platforms incredibly powerful used bring criminals terrorists nation state propaganda programs closer targets easily connect groups friends tools taken lightly managed fashion corporate hacks\n",
      "\n",
      "4: different working company surrounded people hierarchy everyone aligned mission company get lot respect outside world working company like facebook lot perks luxuries employees matter much top management commitment company mission employee get resources execute job well also company maintains highest levels transperancy industry access entire product roadmaps dashboard share feedback ideas anything access top management including zuckerberg something relevant discuss share lot flexibility fb offers work home 4 months later ety leaves unlimited sick days etc offer fb seriously evaluate let go\n",
      "\n",
      "5: facebook hyderabad one best facebook offices work good people really friendly everyone takes time know also best facilities nowhere world facebook employee picked doorstep time convenient bad side working facebook hyderabad sometimes one might feel little detached bigger picture facebook creating globally problem specific facebook hyderabad general remote office everyone facebook make effort make people remote offices realize valuable much problem lastly working remote office means opportunity travel various locations surely covers whatever missing\n",
      "\n",
      "6: worked fb contractor speak experience time frame facebook like boot camp prepared career facilities operations manager team supported day day operations facebook also supported special events included limited hackathons f8 product launches visits podus barack obama bush president al gore various celebrities tenure treated well never felt like second class citizen allowed partake various programs allowed eat cafes free participated transportation program attended various functions facebook really tried make everyone feel like part family whenever participated meetings views opinions respected felt like contributed success organization want work facebook opportunity go contractor go\n",
      "\n",
      "7: keep mind facebook outright hire java engineers engineers engineers hired generalists happen work various technologies project requires really java developers facebook like people spend good amount time java aka android etc good sure answer looking looking list tools use make life checkout mobile scale conference facebook hosts checkout video android tools publicly talk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aishika\n",
      "[nltk_data]     Nandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aishika\n",
      "[nltk_data]     Nandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_review(review):\n",
    "    words = word_tokenize(review)\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    summary = ' '.join(filtered_words)\n",
    "    return summary\n",
    "\n",
    "cummulative_pp = ''\n",
    "for review in company_blog_posts:\n",
    "    summary = process_review(review)\n",
    "    cummulative_pp += summary + ' '\n",
    "\n",
    "individual_post_pp = [process_review(review) for review in company_blog_posts]\n",
    "\n",
    "# Display individual summaries\n",
    "print(\"\\nPreprocessed Individual Blogs\")\n",
    "for i, summary in enumerate(individual_post_pp, start=1):\n",
    "    print(f\"{i}: {summary}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60e770-78f5-4b25-abe2-b375cbe5fd41",
   "metadata": {},
   "source": [
    "## Summarizer (Pre-Trained Bart-Large model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dda30-3e00-4c40-85c1-057a4b93ae53",
   "metadata": {},
   "source": [
    "### Summarisation (raw blogs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fceda30b-ea4d-41fc-a9d3-9c61a2b0cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day-to-day environment at Facebook is unstructured. No one really tells you what to do and when to do it. It's up to you to hustle to make sure you're invited to the right meetings and spending your time wisely.\n",
      "I manage a team of Data Scientists and analysts working on Ads. We belong to probably the largest and most centralized analytics team at Facebook. Our goal is to come up with data backed insights which will result in informing the product road-map.\n",
      "I recently worked at Facebook. It was horrendous. I can’t speak for the entire company, but my organization was a disaster. The pay and benefits were outstanding, so please believe me when I say that I quit because each day felt like mental torture.\n",
      "Facebook has the highest levels of transperancy in the industry. You will get all the resources to execute your job well. There’s a lot of flexibility that FB offers - work from home, 4 months later Ety leaves, unlimited sick days etc.\n",
      "Facebook Hyderabad is one of the best Facebook offices to work at. It's good because people are really friendly and everyone takes time to know each other. Nowhere in the world, a Facebook employee is picked up from their doorstep.\n",
      "I worked at FB as a contractor from 2010-2012. Facebook for me was like boot camp that prepared me for my career. I never felt like a second class citizen and was allowed to partake in various programs. If you want to work at Facebook and have an opportunity to go as a contractors, go for it!\n",
      "Facebook does not outright hire \"Java Engineers\" or \"C++ Engineers\" Engineers are hired as generalists, and then happen to work in various technologies. So there really are no \"Java Developers\" at Facebook.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set up the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# Initialize an empty list to store summaries\n",
    "summaries = []\n",
    "\n",
    "# Process each post\n",
    "for post in company_blog_posts:\n",
    "    # Tokenize the post\n",
    "    tokens = word_tokenize(post)\n",
    "\n",
    "    summary_chunk = summarizer(post, max_length=len(tokens), min_length=min(30, len(tokens)), do_sample=False)\n",
    "    summaries.append(summary_chunk[0]['summary_text'])\n",
    "\n",
    "# Print or use the generated summaries\n",
    "for summary in summaries:\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2dc30c-ab7b-491d-be77-01906d51494b",
   "metadata": {},
   "source": [
    "### Summarisation on preprocessed blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80254a05-f483-4dac-84aa-e3f782835c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six months started walked around completely astonished lucky wrapped bubble gratitude eventually perks start feel normal although people talk often reason stuck around four years hustle never worked harder life facebook put long days often logged weekends felt tremendous sense responsibility good job environment.\n",
      "Data scientist responsibilities spend time analyzing designing experiments optimize product features move key metrics. Data come business opportunities pursue product feature suggestions sometimes understand metric movements. Building production ml models though mostly done sw engineering multidisciplinary nature.\n",
      " recently worked facebook horrendous speak entire company organization disaster pay benefits outstanding please believe say quit day felt like mental torture starters gross ineptitude extended way management chain people worked idea apart distributing corporate acting like used car salesmen help lower level employees.\n",
      "Facebook offers a lot of perks and luxuries. The company maintains the highest levels of transperancy in the industry. Employees matter much to the company's mission.\n",
      " facebook hyderabad one best facebook offices work good people really friendly everyone takes time know also best facilities nowhere world. working remote office means opportunity travel various locations surely covers whatever missing.\n",
      " facebook really tried make everyone feel like part family whenever participated meetings views opinions respected felt like contributed success. worked fb contractor speak experience time frame facebook like boot camp prepared career facilities operations manager team supported day day operations facebook also supported special events included limited hackathons.\n",
      " facebook outright hire java engineers engineers engineers hired generalists happen work various technologies project requires really java developers facebook like people spend good amount time java aka android etc good sure answer looking looking list tools use make life checkout mobile scale conference facebook hosts checkout video android tools\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set up the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# Initialize an empty list to store summaries\n",
    "summaries = []\n",
    "\n",
    "# Process each post\n",
    "for i in individual_post_pp:\n",
    "    # Tokenize the post\n",
    "    tokens = word_tokenize(i)\n",
    "\n",
    "    # Summarize the last chunk\n",
    "    summary_chunk = summarizer(i, max_length=max(min(150,  len(tokens)),min(30,min(30, len(tokens)))), min_length=min(30,min(30, len(tokens))), do_sample=False)\n",
    "    summaries.append(summary_chunk[0]['summary_text'])\n",
    "\n",
    "# Print or use the generated summaries\n",
    "for summary in summaries:\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dbcf2-f743-4218-becb-f00ee2b4f472",
   "metadata": {},
   "source": [
    "# Training Model for summarisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7edbe856-f6c8-4520-b79e-3ccf6cbc6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.20.0\n",
      "  Using cached transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.0)\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.20.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers==4.20.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.20.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.20.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.20.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.20.0) (2023.7.22)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_nlp==0.3.0\n",
      "  Using cached keras_nlp-0.3.0-py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras_nlp==0.3.0) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras_nlp==0.3.0) (1.24.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras_nlp==0.3.0) (23.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras_nlp==0.3.0) (2.14.0)\n",
      "INFO: pip is looking at multiple versions of keras-nlp to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-text (from keras-nlp) (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (2023.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub) (2023.7.22)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge-score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aishika nandi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user transformers==4.20.0\n",
    "!pip install --user keras_nlp==0.3.0\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92d6a7a-ab67-4b01-9b34-aa88c4b835e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0edce4d7-0ce4-4334-9c66-28f9a128d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.1  \n",
    "\n",
    "MAX_INPUT_LENGTH = 1024  \n",
    "MIN_TARGET_LENGTH = 5  \n",
    "MAX_TARGET_LENGTH = 128  \n",
    "BATCH_SIZE = 8         \n",
    "LEARNING_RATE = 2e-5    \n",
    "MAX_EPOCHS = 2          \n",
    "\n",
    "MODEL_CHECKPOINT = \"t5-small\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae35c44-69bb-4c25-9c74-9e15bf668540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'National Archives \\n \\n Yes, it’s that time again, folks. It’s the first Friday of the month, when for one ever-so-brief moment the interests of Wall Street, Washington and Main Street are all aligned on one thing: Jobs. \\n \\n A fresh update on the U.S. employment situation for January hits the wires at 8:30 a.m. New York time offering one of the most important snapshots on how the economy fared during the previous month. Expectations are for 203,000 new jobs to be created, according to economists polled by Dow Jones Newswires, compared to 227,000 jobs added in February. The unemployment rate is expected to hold steady at 8.3%. \\n \\n Here at MarketBeat HQ, we’ll be offering color commentary before and after the data crosses the wires. Feel free to weigh-in yourself, via the comments section. And while you’re here, why don’t you sign up to follow us on Twitter. \\n \\n Enjoy the show. ||||| Employers pulled back sharply on hiring last month, a reminder that the U.S. economy may not be growing fast enough to sustain robust job growth. The unemployment rate dipped, but mostly because more Americans stopped looking for work. \\n \\n The Labor Department says the economy added 120,000 jobs in March, down from more than 200,000 in each of the previous three months. \\n \\n The unemployment rate fell to 8.2 percent, the lowest since January 2009. The rate dropped because fewer people searched for jobs. The official unemployment tally only includes those seeking work. \\n \\n The economy has added 858,000 jobs since December _ the best four months of hiring in two years. But Federal Reserve Chairman Ben Bernanke has cautioned that the current hiring pace is unlikely to continue without more consumer spending.', 'summary': '– The unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today\\'s jobs report. Reaction on the Wall Street Journal\\'s MarketBeat Blog was swift: \"Woah!!! Bad number.\" The unemployment rate, however, is better news; it had been expected to hold steady at 8.3%. But the AP notes that the dip is mostly due to more Americans giving up on seeking employment.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"multi_news\", split=\"train\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1921bde-62b6-4130-85db-a03d739c3e44",
   "metadata": {},
   "source": [
    "## Training-Testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04fec6ee-2ac7-466b-b968-8d466ed888ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = dataset.train_test_split(\n",
    "    train_size=TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",
    ")\n",
    "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be37030-853d-4847-89f0-cfc02d5af865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"]\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "053e0fc6-de72-4fed-9820-7bc9a0761519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a91c5e1db4be597dc2ffba61c4743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbbbfeb091343019ed401e0a38188b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926ed39-ec1a-4108-b0fa-52a34763eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46bbab3e-31c5-4a81-a544-e4c84fedc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c097a-5307-4bf8-b998-a0459d4db6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "generation_dataset = (\n",
    "    tokenized_datasets[\"test\"]\n",
    "    .shuffle()\n",
    "    .select(list(range(200)))\n",
    "    .to_tf_dataset(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec979155-b356-4fd6-9bf1-e5b2d0e125aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e879a-964b-4e9c-93c5-80c19de71010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "\n",
    "rouge_l = keras_nlp.metrics.RougeL()\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_l(decoded_labels, decoded_predictions)\n",
    "    # We will print only the F1 score, you can use other aggregation metrics as well\n",
    "    result = {\"RougeL\": result[\"f1_score\"]}\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51865abb-22b2-43b6-a019-58eaa6ddc2ec",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5b401-a2ef-4445-a835-17bff32dde9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
    ")\n",
    "save_dir = \"/kaggle/working/saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(save_dir, 'model_epoch_{epoch:02d}.h5'),  # No monitor parameter\n",
    "    save_best_only=False,  # Save on every epoch\n",
    "    save_weights_only=False,  # Save entire model\n",
    "    save_freq='epoch',  # Save on every epoch\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n",
    "# For now we will use our test set as our validation_data\n",
    "model.fit(\n",
    "    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e903a4-6f91-4c75-b477-152d30add024",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7731f95-f095-4045-bd27-53352c953b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "print(\"Actual Data\",raw_datasets[\"test\"][0][\"document\"])\n",
    "print(\"\\n-----\\n\")\n",
    "summarizer(\n",
    "    raw_datasets[\"test\"][0][\"document\"],\n",
    "    min_length=MIN_TARGET_LENGTH,\n",
    "    max_length=MAX_TARGET_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78e56a-12be-4c0d-bb81-1a9d2514cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same model used in Reputation Analysis from news. Facing compatibility issues in transformers and keras, will submit by next phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
