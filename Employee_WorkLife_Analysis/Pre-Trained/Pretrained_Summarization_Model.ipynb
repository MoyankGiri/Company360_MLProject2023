{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":29928,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Transformer for Summarization","metadata":{}},{"cell_type":"markdown","source":"\n### Introduction\n\n1. ***Extractive Summary:*** the network calculates the most important sentences from the article and gets them together to provide the most meaningful information from the article.\n2. ***Abstractive Summary***: The network creates new sentences to encapsulate maximum gist of the article and generates that as output. The sentences in the summary may or may not be contained in the article. \n\nWe will be generating ***Abstractive Summary***. \n","metadata":{}},{"cell_type":"code","source":"!pip install transformers -q\n\n# Code for TPU packages install\n# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"id":"WD_vnyLXZQzD","outputId":"b2ff57b8-a147-4893-80bd-e40d18042f98","execution":{"iopub.status.busy":"2023-11-30T04:51:11.426747Z","iopub.execute_input":"2023-11-30T04:51:11.427134Z","iopub.status.idle":"2023-11-30T04:51:18.977133Z","shell.execute_reply.started":"2023-11-30T04:51:11.427104Z","shell.execute_reply":"2023-11-30T04:51:18.976180Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: You are using pip version 20.1; however, version 23.3.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing stock libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface/transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration","metadata":{"id":"pzM1_ykHaFur","outputId":"58fa0ba8-b486-4b26-aaea-c0331b343b70","execution":{"iopub.status.busy":"2023-11-30T04:51:18.979364Z","iopub.execute_input":"2023-11-30T04:51:18.979652Z","iopub.status.idle":"2023-11-30T04:51:25.569895Z","shell.execute_reply.started":"2023-11-30T04:51:18.979622Z","shell.execute_reply":"2023-11-30T04:51:25.569026Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # Setting up the device for GPU usage\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\n# Preparing for TPU usage\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()","metadata":{"id":"NLxxwd1scQNv","execution":{"iopub.status.busy":"2023-11-30T04:55:28.661018Z","iopub.execute_input":"2023-11-30T04:55:28.661509Z","iopub.status.idle":"2023-11-30T04:55:28.666357Z","shell.execute_reply.started":"2023-11-30T04:55:28.661463Z","shell.execute_reply":"2023-11-30T04:55:28.665550Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"\n### Our Approach\n- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n- *Training Dataset* is used to fine tune the model: **80% of the original data**\n- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n","metadata":{}},{"cell_type":"code","source":"# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = source_len\n        self.summ_len = summ_len\n        self.text = self.data.text\n        self.ctext = self.data.ctext\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        ctext = str(self.ctext[index])\n        ctext = ' '.join(ctext.split())\n\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"id":"932p8NhxeNw4","execution":{"iopub.status.busy":"2023-11-30T04:55:32.730599Z","iopub.execute_input":"2023-11-30T04:55:32.730927Z","iopub.status.idle":"2023-11-30T04:55:32.744323Z","shell.execute_reply.started":"2023-11-30T04:55:32.730897Z","shell.execute_reply":"2023-11-30T04:55:32.743428Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<a id='section03'></a>\n### Fine Tuning the Model: Function\n\nFollowing events happen in this function to fine tune the neural network:\n- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n- The dataloader passes data to the model based on the batch size.\n- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n- The model outputs first element gives the loss for the forward pass. \n- Loss value is used to optimize the weights of the neurons in the network.\n- After every 10 steps the loss value is logged in the wandb service. \n- After every 500 steps the loss value is printed in the console.","metadata":{}},{"cell_type":"code","source":"# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n\ndef train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    for _,data in enumerate(loader, 0):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n        loss = outputs[0]\n        \n        if _%500==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # xm.optimizer_step(optimizer)\n        # xm.mark_step()","metadata":{"id":"SaPAR7TWmxoM","execution":{"iopub.status.busy":"2023-11-30T04:55:36.966344Z","iopub.execute_input":"2023-11-30T04:55:36.966670Z","iopub.status.idle":"2023-11-30T04:55:36.977112Z","shell.execute_reply.started":"2023-11-30T04:55:36.966641Z","shell.execute_reply":"2023-11-30T04:55:36.976364Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Validating the Model Performance: Function\n\nDuring the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n","metadata":{}},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for _, data in enumerate(loader, 0):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n\n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=150, \n                num_beams=2,\n                repetition_penalty=2.5, \n                length_penalty=1.0, \n                early_stopping=True\n                )\n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n            if _%100==0:\n                print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n    return predictions, actuals","metadata":{"id":"j9TNdHlQ0CLz","execution":{"iopub.status.busy":"2023-11-30T04:55:43.507717Z","iopub.execute_input":"2023-11-30T04:55:43.508146Z","iopub.status.idle":"2023-11-30T04:55:43.519048Z","shell.execute_reply.started":"2023-11-30T04:55:43.508111Z","shell.execute_reply":"2023-11-30T04:55:43.518299Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### T5 model \n\n* In this stage we define the model and optimizer that will be used for training and to update the weights of the network. \n* We are using the `t5-base` transformer model for our project. \n* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n* We are using the `Adam` optimizer for our project. \n","metadata":{}},{"cell_type":"code","source":"\nTRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\nVALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\nTRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\nVAL_EPOCHS = 1 \nLEARNING_RATE = 1e-4    # learning rate (default: 0.01)\nSEED = 42               # random seed (default: 42)\nMAX_LEN = 512\nSUMMARY_LEN = 150 \n\n# Set random seeds and deterministic pytorch for reproducibility\ntorch.manual_seed(SEED) # pytorch random seed\nnp.random.seed(SEED) # numpy random seed\ntorch.backends.cudnn.deterministic = True\n\n# tokenzier for encoding the text\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n\n# Importing and Pre-Processing the domain data\n# Selecting the needed columns only. \n# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \ndf = pd.read_csv('../input/news-summary/news_summary.csv',encoding='latin-1')\ndf = df[['text','ctext']]\ndf.ctext = 'summarize: ' + df.ctext\nprint(df.head())\n\n\n# Creation of Dataset and Dataloader\n# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \ntrain_size = 0.8\ntrain_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\nval_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(val_dataset.shape))\n\n\n# Creating the Training and Validation dataset for further creation of Dataloader\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\nval_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n\n# Defining the parameters for creation of dataloaders\ntrain_params = {\n    'batch_size': TRAIN_BATCH_SIZE,\n    'shuffle': True,\n    'num_workers': 0\n    }\n\nval_params = {\n    'batch_size': VALID_BATCH_SIZE,\n    'shuffle': False,\n    'num_workers': 0\n    }\n\n# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\ntraining_loader = DataLoader(training_set, **train_params)\nval_loader = DataLoader(val_set, **val_params)\n\n\n\n# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n# Further this model is sent to device (GPU/TPU) for using the hardware.\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\nmodel = model.to(device)\n\n# Defining the optimizer that will be used to tune the weights of the network in the training session. \noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n\n# Training loop\nprint('Initiating Fine-Tuning for the model on our dataset')\n\nfor epoch in range(TRAIN_EPOCHS):\n    train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n\n# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n# Saving the dataframe as predictions.csv\nprint('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\nfor epoch in range(VAL_EPOCHS):\n    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n    final_df.to_csv('predictions.csv')\n    print('Output Files generated for review')\n","metadata":{"id":"ZtNs9ytpCow2","outputId":"80545587-0a82-455a-a9ba-13eb3fcb1550","execution":{"iopub.status.busy":"2023-11-30T07:41:04.318283Z","iopub.execute_input":"2023-11-30T07:41:04.318667Z","iopub.status.idle":"2023-11-30T08:28:22.861304Z","shell.execute_reply.started":"2023-11-30T07:41:04.318638Z","shell.execute_reply":"2023-11-30T08:28:22.860410Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"                                                text  \\\n0  The Administration of Union Territory Daman an...   \n1  Malaika Arora slammed an Instagram user who tr...   \n2  The Indira Gandhi Institute of Medical Science...   \n3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n4  Hotels in Maharashtra will train their staff t...   \n\n                                               ctext  \n0  summarize: The Daman and Diu administration on...  \n1  summarize: From her special numbers to TV?appe...  \n2  summarize: The Indira Gandhi Institute of Medi...  \n3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n4  summarize: Hotels in Mumbai and other Indian c...  \nFULL Dataset: (4514, 2)\nTRAIN Dataset: (3611, 2)\nTEST Dataset: (903, 2)\nInitiating Fine-Tuning for the model on our dataset\nEpoch: 0, Loss:  5.861971378326416\nEpoch: 0, Loss:  1.909801959991455\nEpoch: 0, Loss:  1.4936964511871338\nEpoch: 0, Loss:  1.9448661804199219\nEpoch: 1, Loss:  1.990925908088684\nEpoch: 1, Loss:  1.2291879653930664\nEpoch: 1, Loss:  1.1979399919509888\nEpoch: 1, Loss:  1.6737912893295288\nNow generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\nCompleted 0\nCompleted 100\nCompleted 200\nCompleted 300\nCompleted 400\nOutput Files generated for review\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# Load pretrained T5 model and tokenizer\npretrained_model_name = \"t5-base\"\nmodel = T5ForConditionalGeneration.from_pretrained(pretrained_model_name)\ntokenizer = T5Tokenizer.from_pretrained(pretrained_model_name)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Example text for summarization\ninput_text = \"An Air India flight was grounded after a catering van hit the door of an aircraft at Delhi Airport on Sunday morning, according to reports. Air India officials assessing the damage said the loss will run into lakhs. The flight was delayed by an hour, while the airport driving permit of the van's driver was seized.\"\n\n# Tokenize and generate summary\ninput_ids = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\nsummary_ids = model.generate(input_ids, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n\n# Decode the generated summary\ngenerated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(\"Generated Summary:\", generated_summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T08:38:45.946603Z","iopub.execute_input":"2023-11-30T08:38:45.946907Z","iopub.status.idle":"2023-11-30T08:38:54.375986Z","shell.execute_reply.started":"2023-11-30T08:38:45.946878Z","shell.execute_reply":"2023-11-30T08:38:54.375205Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Generated Summary: catering van hits door of aircraft at Delhi airport, reports say. flight delayed by an hour while driver's license is seized - airline says loss could run into lakh dollars in damages to aircraft if not repaired a day later...\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Save the model locally\nsave_directory = \"./\"\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T08:39:00.952495Z","iopub.execute_input":"2023-11-30T08:39:00.952831Z","iopub.status.idle":"2023-11-30T08:39:04.217426Z","shell.execute_reply.started":"2023-11-30T08:39:00.952801Z","shell.execute_reply":"2023-11-30T08:39:04.216467Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"('./spiece.model', './special_tokens_map.json', './added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\nspiece_model_path = './spiece.model'\nspecial_tokens_map_path = './special_tokens_map.json'\nadded_tokens_path = './added_tokens.json'\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel_directory = './'\n\n# Load the T5 model\nloaded_model = T5ForConditionalGeneration.from_pretrained(model_directory)\nloaded_tokenizer = T5Tokenizer.from_pretrained(model_directory)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T08:46:27.469858Z","iopub.execute_input":"2023-11-30T08:46:27.470528Z","iopub.status.idle":"2023-11-30T08:46:34.179278Z","shell.execute_reply.started":"2023-11-30T08:46:27.470474Z","shell.execute_reply":"2023-11-30T08:46:34.178532Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def generate_summary(input_text, tokenizer, model, max_length=150):\n    inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    outputs = model.generate(inputs, max_length=max_length, num_beams=4, length_penalty=2.0, early_stopping=True)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary\n\n# Replace loaded_tokenizer and loaded_model with your loaded tokenizer and model\ninput_text = input_text = \"An Air India flight was grounded after a catering van hit the door of an aircraft at Delhi Airport on Sunday morning, according to reports. Air India officials assessing the damage said the loss will run into lakhs. The flight was delayed by an hour, while the airport driving permit of the van's driver was seized.\"\n\nsummary = generate_summary(input_text, loaded_tokenizer, loaded_model)\n\nprint(\"Original Text:\")\nprint(input_text)\nprint(\"\\nGenerated Summary:\")\nprint(summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T08:46:34.181109Z","iopub.execute_input":"2023-11-30T08:46:34.181473Z","iopub.status.idle":"2023-11-30T08:46:38.185771Z","shell.execute_reply.started":"2023-11-30T08:46:34.181434Z","shell.execute_reply":"2023-11-30T08:46:38.184691Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Original Text:\nAn Air India flight was grounded after a catering van hit the door of an aircraft at Delhi Airport on Sunday morning, according to reports. Air India officials assessing the damage said the loss will run into lakhs. The flight was delayed by an hour, while the airport driving permit of the van's driver was seized.\n\nGenerated Summary:\ncatering van hit the door of an aircraft at Delhi airport on Sunday morning. the flight was delayed by an hour, while the airport driving permit of the van's driver was seized. air india officials assessing the damage said the loss will run into lakhs.\n","output_type":"stream"}]}]}